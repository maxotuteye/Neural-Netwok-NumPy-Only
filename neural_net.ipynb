{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "neural-net.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxotuteye/MLandAI/blob/main/neural_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuB3FxhsMPIx"
      },
      "source": [
        "This notebook demonstrates how we can build a working neural net using NumPy library only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTSkABWXMPI9"
      },
      "source": [
        "# 1. Initialize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.374378Z",
          "start_time": "2019-05-18T21:14:27.365566Z"
        },
        "id": "4o1kgpLjMPI-"
      },
      "source": [
        "import numpy as np \n",
        "np.random.seed(42) # for reproducibility"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nwMvbLPMPI-"
      },
      "source": [
        "# 2. Generate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.400704Z",
          "start_time": "2019-05-18T21:14:27.377332Z"
        },
        "id": "JRMnsMM9MPI_",
        "outputId": "99e495e1-d0f0-4dc1-a861-25580d3aab24"
      },
      "source": [
        "X_num_row, X_num_col = [2, 10000] # Row is no. of feature, col is no. of datum points\n",
        "X_raw = np.random.rand(X_num_row,X_num_col) * 100\n",
        "X_raw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[37.45401188, 95.07143064, 73.19939418, ..., 94.6707915 ,\n",
              "        39.74879924, 21.7140404 ],\n",
              "       [37.36408185, 33.29120962, 17.61539125, ..., 30.36984691,\n",
              "        44.33200065, 17.22648144]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.411696Z",
          "start_time": "2019-05-18T21:14:27.403128Z"
        },
        "id": "xc5m2hSSMPJC",
        "outputId": "1ef9d463-06f9-49f3-8f47-6db84368a09a"
      },
      "source": [
        "y_raw = np.concatenate(([(X_raw[0,:] + X_raw[1,:])], [(X_raw[0,:] - X_raw[1,:])], np.abs([(X_raw[0,:] - X_raw[1,:])])))\n",
        "# for input a and b, output is a+b; a-b and |a-b|\n",
        "y_num_row, y_num_col = y_raw.shape\n",
        "y_raw.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmplHm3dMPJD"
      },
      "source": [
        "# 3. Split train-test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.419829Z",
          "start_time": "2019-05-18T21:14:27.413643Z"
        },
        "id": "Uh0QDDwqMPJD"
      },
      "source": [
        "train_ratio = 0.7\n",
        "num_train_datum = int(train_ratio*X_num_col)\n",
        "X_raw_train = X_raw[:,0:num_train_datum]\n",
        "X_raw_test = X_raw[:,num_train_datum:]\n",
        "\n",
        "\n",
        "y_raw_train = y_raw[:,0:num_train_datum]\n",
        "y_raw_test = y_raw[:,num_train_datum:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "556d18ffMPJE"
      },
      "source": [
        "# 4. Standardize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.462358Z",
          "start_time": "2019-05-18T21:14:27.422168Z"
        },
        "id": "3B6_-e6KMPJE"
      },
      "source": [
        "class scaler:\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "def get_scaler(row):\n",
        "    mean = np.mean(row)\n",
        "    std = np.std(row)\n",
        "    return scaler(mean, std)\n",
        "\n",
        "def standardize(data, scaler):\n",
        "    return (data - scaler.mean) / scaler.std\n",
        "\n",
        "def unstandardize(data, scaler):\n",
        "    return (data * scaler.std) + scaler.mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.472718Z",
          "start_time": "2019-05-18T21:14:27.464724Z"
        },
        "id": "gfXwzKEMMPJF"
      },
      "source": [
        "# Construct scalers from training set\n",
        "\n",
        "X_scalers = [get_scaler(X_raw_train[row,:]) for row in range(X_num_row)]\n",
        "X_train = np.array([standardize(X_raw_train[row,:], X_scalers[row]) for row in range(X_num_row)])\n",
        "\n",
        "y_scalers = [get_scaler(y_raw_train[row,:]) for row in range(y_num_row)]\n",
        "y_train = np.array([standardize(y_raw_train[row,:], y_scalers[row]) for row in range(y_num_row)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.478689Z",
          "start_time": "2019-05-18T21:14:27.474370Z"
        },
        "id": "QxsqbKBVMPJG"
      },
      "source": [
        "# Apply those scalers to testing set\n",
        "\n",
        "X_test = np.array([standardize(X_raw_test[row,:], X_scalers[row]) for row in range(X_num_row)])\n",
        "\n",
        "y_test = np.array([standardize(y_raw_test[row,:], y_scalers[row]) for row in range(y_num_row)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.488529Z",
          "start_time": "2019-05-18T21:14:27.480426Z"
        },
        "id": "S5nFDzMXMPJG",
        "outputId": "49806624-698a-44b3-837d-4c574cd311a1"
      },
      "source": [
        "# Check if data has been standardized\n",
        "\n",
        "print([X_train[row,:].mean() for row in range(X_num_row)]) # should be close to zero\n",
        "print([X_train[row,:].std() for row in range(X_num_row)])  # should be close to one\n",
        "\n",
        "print([y_train[row,:].mean() for row in range(y_num_row)]) # should be close to zero\n",
        "print([y_train[row,:].std() for row in range(y_num_row)])  # should be close to one"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.740664837931815e-16, 5.227564413092166e-17]\n",
            "[0.9999999999999999, 1.0]\n",
            "[-4.608377171929793e-16, 1.0658141036401503e-17, 6.471014200672341e-18]\n",
            "[0.9999999999999999, 1.0, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgfxLxgWMPJH"
      },
      "source": [
        "# 5. Construct a neural net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.506539Z",
          "start_time": "2019-05-18T21:14:27.491123Z"
        },
        "id": "ukHo0EoyMPJH"
      },
      "source": [
        "class layer:\n",
        "    def __init__(self, layer_index, is_output, input_dim, output_dim, activation):\n",
        "        self.layer_index = layer_index # zero indicates input layer\n",
        "        self.is_output = is_output # true indicates output layer, false otherwise\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.activation = activation\n",
        "        \n",
        "        # the multiplication constant is sorta arbitrary\n",
        "        if layer_index != 0:\n",
        "            self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2/input_dim) \n",
        "            self.b = np.random.randn(output_dim, 1) * np.sqrt(2/input_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.526822Z",
          "start_time": "2019-05-18T21:14:27.508751Z"
        },
        "id": "pWVUpk9nMPJI"
      },
      "source": [
        "# Change layers_dim to configure your own neural net!            \n",
        "layers_dim = [X_num_row, 4, 4, y_num_row] # input layer --- hidden layers --- output layers\n",
        "neural_net = []\n",
        "\n",
        "# Construct the net layer by layer\n",
        "for layer_index in range(len(layers_dim)):\n",
        "    if layer_index == 0: # if input layer\n",
        "        neural_net.append(layer(layer_index, False, 0, layers_dim[layer_index], 'irrelevant'))\n",
        "    elif layer_index+1 == len(layers_dim): # if output layer\n",
        "        neural_net.append(layer(layer_index, True, layers_dim[layer_index-1], layers_dim[layer_index], activation='linear'))\n",
        "    else: \n",
        "        neural_net.append(layer(layer_index, False, layers_dim[layer_index-1], layers_dim[layer_index], activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:40:36.862788Z",
          "start_time": "2019-05-18T21:40:36.851594Z"
        },
        "id": "b7aPI6iLMPJI",
        "outputId": "344c5c16-06ff-4d71-af5e-de8052496a8f"
      },
      "source": [
        "# Simple check on overfitting\n",
        "\n",
        "pred_n_param = sum([(layers_dim[layer_index]+1)*layers_dim[layer_index+1] for layer_index in range(len(layers_dim)-1)])\n",
        "act_n_param = sum([neural_net[layer_index].W.size + neural_net[layer_index].b.size for layer_index in range(1,len(layers_dim))])\n",
        "print(f'Predicted number of hyperparameters: {pred_n_param}')\n",
        "print(f'Actual number of hyperparameters: {act_n_param}')\n",
        "print(f'Number of data: {X_num_col}')\n",
        "\n",
        "if act_n_param >= X_num_col:\n",
        "    raise Exception('It will overfit.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted number of hyperparameters: 47\n",
            "Actual number of hyperparameters: 47\n",
            "Number of data: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqW571S_MPJJ"
      },
      "source": [
        "# 6. Perform forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.564092Z",
          "start_time": "2019-05-18T21:14:27.558739Z"
        },
        "id": "OqWGbafeMPJK"
      },
      "source": [
        "def activation(input_, act_func):\n",
        "    if act_func == 'relu':\n",
        "        return np.maximum(input_, np.zeros(input_.shape))\n",
        "    elif act_func == 'linear':\n",
        "        return input_\n",
        "    else:\n",
        "        raise Exception('Activation function is not defined.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.572809Z",
          "start_time": "2019-05-18T21:14:27.566185Z"
        },
        "id": "xNs6AGdMMPJK"
      },
      "source": [
        "def forward_prop(input_vec, layers_dim=layers_dim, neural_net=neural_net):\n",
        "    neural_net[0].A = input_vec # Define A in input layer for for-loop convenience\n",
        "    for layer_index in range(1,len(layers_dim)): # W,b,Z,A are undefined in input layer\n",
        "        neural_net[layer_index].Z = np.add(np.dot(neural_net[layer_index].W, neural_net[layer_index-1].A), neural_net[layer_index].b)\n",
        "        neural_net[layer_index].A = activation(neural_net[layer_index].Z, neural_net[layer_index].activation)\n",
        "    return neural_net[layer_index].A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.812885Z",
          "start_time": "2019-05-18T21:14:27.574665Z"
        },
        "id": "mxGON9JkMPJL",
        "outputId": "a01aefdb-e27e-4f58-ba54-d8a892edc9d9"
      },
      "source": [
        "# test run\n",
        "\n",
        "forward_prop(X_train).shape == y_train.shape # should be True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEz7B1cVMPJL"
      },
      "source": [
        "# 7. Perform back propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.843912Z",
          "start_time": "2019-05-18T21:14:27.815170Z"
        },
        "id": "doflyIc0MPJL"
      },
      "source": [
        "def get_loss(y, y_hat, metric='mse'):\n",
        "    if metric == 'mse':\n",
        "        individual_loss = 0.5 * (y_hat - y) ** 2\n",
        "        return np.mean([np.linalg.norm(individual_loss[:,col], 2) for col in range(individual_loss.shape[1])])\n",
        "    else:\n",
        "        raise Exception('Loss metric is not defined.')\n",
        "\n",
        "def get_dZ_from_loss(y, y_hat, metric):\n",
        "    if metric == 'mse':\n",
        "        return y_hat - y\n",
        "    else:\n",
        "        raise Exception('Loss metric is not defined.')\n",
        "\n",
        "def get_dactivation(A, act_func):\n",
        "    if act_func == 'relu':\n",
        "        return np.maximum(np.sign(A), np.zeros(A.shape)) # 1 if backward input >0, 0 otherwise; then diaganolize\n",
        "    elif act_func == 'linear':\n",
        "        return np.ones(A.shape)\n",
        "    else:\n",
        "        raise Exception('Activation function is not defined.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:14:27.870828Z",
          "start_time": "2019-05-18T21:14:27.847510Z"
        },
        "id": "kcuO0OdDMPJM"
      },
      "source": [
        "def backward_prop(y, y_hat, metric='mse', layers_dim=layers_dim, neural_net=neural_net, num_train_datum=num_train_datum):\n",
        "    for layer_index in range(len(layers_dim)-1,0,-1):\n",
        "        if layer_index+1 == len(layers_dim): # if output layer\n",
        "            dZ = get_dZ_from_loss(y, y_hat, metric)\n",
        "        else: \n",
        "            dZ = np.multiply(np.dot(neural_net[layer_index+1].W.T, dZ), \n",
        "                             get_dactivation(neural_net[layer_index].A, neural_net[layer_index].activation))\n",
        "        dW = np.dot(dZ, neural_net[layer_index-1].A.T) / num_train_datum\n",
        "        db = np.sum(dZ, axis=1, keepdims=True) / num_train_datum\n",
        "        \n",
        "        neural_net[layer_index].dW = dW\n",
        "        neural_net[layer_index].db = db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMYLEku_MPJO"
      },
      "source": [
        "# 8. Optimize Iteratively (Gradient Descent)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:31:57.379056Z",
          "start_time": "2019-05-18T21:14:27.873523Z"
        },
        "id": "DlrCJ1MEMPJO",
        "outputId": "615c7c5f-097c-465a-a21c-643ea8345a93"
      },
      "source": [
        "learning_rate = 0.01\n",
        "max_epoch = 1000000\n",
        "\n",
        "for epoch in range(1,max_epoch+1):\n",
        "    y_hat_train = forward_prop(X_train) # update y_hat\n",
        "    backward_prop(y_train, y_hat_train) # update (dW,db)\n",
        "    \n",
        "    for layer_index in range(1,len(layers_dim)):        # update (W,b)\n",
        "        neural_net[layer_index].W = neural_net[layer_index].W - learning_rate * neural_net[layer_index].dW\n",
        "        neural_net[layer_index].b = neural_net[layer_index].b - learning_rate * neural_net[layer_index].db\n",
        "    \n",
        "    if epoch % 100000 == 0:\n",
        "        print(f'{get_loss(y_train, y_hat_train):.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3502\n",
            "0.3450\n",
            "0.3450\n",
            "0.3450\n",
            "0.3450\n",
            "0.3450\n",
            "0.3450\n",
            "0.3450\n",
            "0.3450\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ea6a7e16c47a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_hat_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update y_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbackward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update (dW,db)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m        \u001b[0;31m# update (W,b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-c68e6071bcf8>\u001b[0m in \u001b[0;36mbackward_prop\u001b[0;34m(y, y_hat, metric, layers_dim, neural_net, num_train_datum)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             dZ = np.multiply(np.dot(neural_net[layer_index+1].W.T, dZ), \n\u001b[0;32m----> 7\u001b[0;31m                              get_dactivation(neural_net[layer_index].A, neural_net[layer_index].activation))\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_index\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_train_datum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_train_datum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-7cd25e5d277d>\u001b[0m in \u001b[0;36mget_dactivation\u001b[0;34m(A, act_func)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mact_func\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1 if backward input >0, 0 otherwise; then diaganolize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mact_func\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'linear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2m0VQjBMPJP"
      },
      "source": [
        "# 9. Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:31:57.414111Z",
          "start_time": "2019-05-18T21:31:57.382068Z"
        },
        "id": "y8or3_fDMPJQ",
        "outputId": "58613378-aa7c-4205-d8b7-cd6c645ff1e2"
      },
      "source": [
        "# test loss\n",
        "\n",
        "get_loss(y_test, forward_prop(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.319423956442085"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:31:57.427484Z",
          "start_time": "2019-05-18T21:31:57.416673Z"
        },
        "id": "vQhG_F7GMPJR"
      },
      "source": [
        "def predict(X_raw_any):\n",
        "    X_any = np.array([standardize(X_raw_any[row,:], X_scalers[row]) for row in range(X_num_row)])\n",
        "    y_hat = forward_prop(X_any)\n",
        "    y_hat_any = np.array([unstandardize(y_hat[row,:], y_scalers[row]) for row in range(y_num_row)])\n",
        "    return y_hat_any"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-18T21:31:57.444148Z",
          "start_time": "2019-05-18T21:31:57.430541Z"
        },
        "id": "4qhoDWa2MPJS",
        "outputId": "cf2a26a8-8231-4d4e-e637-85b30f3afb13"
      },
      "source": [
        "predict(np.array([[30,70],[70,30],[3,5],[888,122]]).T)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 98.56377603, 102.6322368 ,   5.5634535 , 562.11918614],\n",
              "       [-26.25106094,  14.80173245,  21.32446122, 221.56716875],\n",
              "       [ 48.08518158,  25.18202845,  15.71606633, -63.99315275]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBADnk42MPJT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}